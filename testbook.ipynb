{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from ollama import chat\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model(prompt, sysprompt=None, model='llama3.2', get_tok_sec=False):\n",
    "    start_time = time.time()\n",
    "    messages = []\n",
    "    if sysprompt:\n",
    "        messages.append({'role': 'system', 'content': sysprompt})\n",
    "    messages.append({'role': 'user', 'content': prompt})\n",
    "    \n",
    "    response = chat(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    message = response.message.content\n",
    "    generation_time = end_time - start_time\n",
    "    input_length = response.prompt_eval_count\n",
    "    if get_tok_sec:\n",
    "        token_count = response.eval_count\n",
    "        tokens_per_second = token_count / generation_time if generation_time > 0 else 0\n",
    "        tokens_per_second = round(tokens_per_second, 2)\n",
    "    else:\n",
    "        tokens_per_second = None\n",
    "\n",
    "    return {\n",
    "        'message': message,\n",
    "        'input_length': input_length,\n",
    "        'gen_time_sec': round(generation_time, 2),\n",
    "        'tok_sec': tokens_per_second\n",
    "    }\n",
    "\n",
    "def load_and_split_text(filename, chunk_size=5000):\n",
    "    file_path = os.path.join('texts', 'keeps', filename)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    text = text.replace('_', '')\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     \"How are you today?\",\n",
    "# ]\n",
    "# system_prompt = \"You are a professor of philsophy who is exceptional at summarizing longer works into the underlying philosophical themes. When you summarize, you describe in a clear and concise manner that fully conveys the argument of the work to other philosophers. Your responses are not for average person, but are incredibly clear to the philosophical expert.\"\n",
    "# # Can choose between 'llama3.2' (~50 tok/s) and 'llama3.2:1b' (~90 tok/s)\n",
    "# model = 'llama3.2:1b'\n",
    "\n",
    "# speeds = []\n",
    "# for prompt in prompts:\n",
    "#     response = chat_with_model(prompt, sysprompt=system_prompt, model=model, get_tok_sec=True)\n",
    "#     print(f\"{prompt}\\n{'='*100}\\n{response['message']}\\n\\n\\n\")\n",
    "#     speeds.append(response['tok_sec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up Online Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = emb_model.encode([\"Your longer text here\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   5/ 148]: Avg. Length: 990.60 | Avg. Speed: 89.82 tok/s | Refusals: 0\n",
      "[  10/ 148]: Avg. Length: 971.60 | Avg. Speed: 91.14 tok/s | Refusals: 3\n",
      "[  15/ 148]: Avg. Length: 1014.80 | Avg. Speed: 90.77 tok/s | Refusals: 1\n",
      "[  20/ 148]: Avg. Length: 979.60 | Avg. Speed: 69.86 tok/s | Refusals: 6\n",
      "[  25/ 148]: Avg. Length: 989.00 | Avg. Speed: 70.20 tok/s | Refusals: 5\n",
      "[  30/ 148]: Avg. Length: 874.60 | Avg. Speed: 90.08 tok/s | Refusals: 2\n",
      "Cleaned 30 inputs and saw 17 refusals\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    'montaigne_ofexperience.txt'\n",
    "]\n",
    "\n",
    "system_prompt = \"You are an expert at cleaning text files, and I will be giving you partial text from Montaigne's essay 'Of Experience'. For your task, you must modernize the spelling. You must also remove any non-english, as he likes to write in Latin at times. Other than making these two changes, do not change anything else. Do not add anything that was not previously there. Do not say anything else. Do not tell me you cleaned it. Do not ask if I need anything else.\"\n",
    "# Can choose between 'llama3.2' (~50 tok/s) and 'llama3.2:1b' (~90 tok/s)\n",
    "model = 'llama3.2:1b'\n",
    "total_refusals = 0\n",
    "\n",
    "for file in files:\n",
    "    text_chunks = load_and_split_text(file, chunk_size=1000)\n",
    "    cleaned_chunks = []\n",
    "    lengths, speeds = [], []\n",
    "    c, refusals = 0, 0\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        og_emb = emb_model.encode(chunk)\n",
    "        for _ in range(5):\n",
    "            response = chat_with_model(chunk, sysprompt=system_prompt, model=model, get_tok_sec=True)\n",
    "            response_emb = emb_model.encode(response['message'])\n",
    "            if og_emb @ response_emb.T > 0.85:\n",
    "                break\n",
    "            else:\n",
    "                refusals += 1\n",
    "            response = {'message': chunk, 'tok_sec': 0}\n",
    "\n",
    "        cleaned_chunks.append(response['message'])\n",
    "        lengths.append(len(response['message']))\n",
    "        speeds.append(response['tok_sec'])\n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f\"[{i+1:>4}/{len(text_chunks):>4}]: Avg. Length: {sum(lengths)/len(lengths):.2f} | Avg. Speed: {sum(speeds)/len(speeds):.2f} tok/s | Refusals: {refusals}\")\n",
    "            total_refusals += refusals\n",
    "            lengths, speeds, refusals = [], [], 0    \n",
    "            c += 1\n",
    "            if c > 5:\n",
    "                break\n",
    "    break\n",
    "\n",
    "print(f\"Cleaned {len(cleaned_chunks)} inputs and saw {total_refusals} refusals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with one, to which hee may so exactly joyne and match it, but some circumstance and diversitie will remaine, that may require a diverse consideration of judgement. There is but little relation betweene our actions, that are in perpetuall mutation, and the fixed and unmooveable lawes. The most to be desired, are the rarest, the simplest and most generall. And yet I believe, it were better to have none at all, then so infinite a number as we have. Nature gives them ever more happy, then those we give our selves. Witnesse the image of the golden age that Poets faine; and the state wherein we see diverse nations to live, which have no other. Some there are, who to decide any controversie, that may rise amongest them, will chuse for judge the first man that by chance shall travell alongst their mountaines: Others, that upon a market day will name some one amongst themselves, who in the place without more wrangling shall determine all their questions. What danger would ensue, if the wisest \n",
      "----------------------------------------------------------------------------------------------------\n",
      "With one, to which hee may so exactly joyne and match it, but some circumstance and diversitie will remaine, that may require a diverse consideration of judgement.\n",
      "\n",
      "There is but little relation betweene our actions, that are in perpetuall mutation, and the fixed and unmooveable lawes. The most to be desired, are the rarest, the simplest and most generall. And yet I believe, it were better to have none at all, then so infinite a number as we have.\n",
      "\n",
      "Nature gives them ever more happy, then those we give our selves. Witnesse the image of the golden age that Poets faine; and the state wherein we see diverse nations to live, which have no other. Some there are, who to decide any controversie, that may rise amongest them, will chuse for judge the first man that by chance shall travell alongst their mountaines: Others, that upon a market day will name some one amongst themselves, who in the place without more wrangling shall determine all their questions.\n",
      "\n",
      "What danger would ensue, if the wisest weree to be chose as judge, that hath no knowledge of his selfe?\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "print(text_chunks[i])\n",
    "print('-'*100)\n",
    "print(cleaned_chunks[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "philosophy_oracle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
