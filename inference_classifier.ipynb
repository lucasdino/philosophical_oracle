{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Our Classifier for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import loader\n",
    "import models\n",
    "import utility\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def philosophize_this(inference_df, model, device, label_to_category_mapping, print_info=False):\n",
    "\n",
    "    def logits_to_prediction(logits, label_to_category_mapping):\n",
    "        sorted_indices = torch.argsort(logits, descending=True)    \n",
    "        predictions = [(label_to_category_mapping[str(idx.item())], logits[idx].item()) for idx in sorted_indices]\n",
    "        return predictions\n",
    "\n",
    "    full_logits = torch.zeros((len(inference_df), len(label_to_category_mapping)))\n",
    "    for i, row in inference_df.iterrows():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(row['embedding'].to(device)).cpu()\n",
    "            logits = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        full_logits[i] = logits\n",
    "        predictions = logits_to_prediction(logits, label_to_category_mapping)\n",
    "        \n",
    "        if print_info:\n",
    "            print(f\"{[f'{c}, {v:.2f}' for c, v in predictions]}\")\n",
    "            print(row['chunk_text'])\n",
    "            print(f\"{'='*100}\\n\")\n",
    "\n",
    "    final_prediction = logits_to_prediction(full_logits.mean(dim=0), label_to_category_mapping)\n",
    "    print(f\"Final philosophical prediction of your input text:\\n{[f'{c}, {v:.2f}' for c, v in final_prediction]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model_weights\\classifier_29k_11242024_191324.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\Desktop\\UCSD\\256\\philosophical_oracle\\models\\classifier.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_info = torch.load(filepath)\n"
     ]
    }
   ],
   "source": [
    "filename_category_mapping, category_label_mapping = loader.load_labeling_mappings()\n",
    "filename_label_mapping = filename_category_mapping\n",
    "for key, value in filename_label_mapping.items():\n",
    "    filename_label_mapping[key] = int(category_label_mapping[value])\n",
    "\n",
    "label_to_category_mapping = {v: k for k, v in category_label_mapping.items()}\n",
    "\n",
    "# Load the model\n",
    "filename = 'classifier_29k_11242024_191324.pth'\n",
    "model = models.Classifier.load_model(filename).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transcendentalism, 0.44', 'absurdism, 0.38', 'epicureanism, 0.15', 'existentialism, 0.02', 'stoicism, 0.01', 'buddhism, 0.00', 'rationalism, 0.00']\n",
      "This is all I took with me:\n",
      "\n",
      "4 shorts, 2 pants, 2 swimsuits\n",
      "7 shirts\n",
      "1 jacket\n",
      "6 pairs of underwear\n",
      "7 pairs of socks\n",
      "One backpack towel, hat, pair of sunglasses\n",
      "3 pairs of shoes\n",
      "Water bottle, day pack, dry sack, water bladder\n",
      "Passport, headphones, laptop, chargers, sketchbook (yellow bag), belt (blue bag)\n",
      "Couple fun things for Croatia music festival in blue bag\n",
      "Lost these Chacos when flying *(RIP)*, picked up a new shirt and shorts for Croatia at a thrift store in Barcelona, bought a pair of sunglasses from a friend’s store in Greece, and got a shirt in Byron Bay at the end of the trip. Otherwise it stayed all the same.\n",
      "If it wasn’t for Croatia and some of the long hikes I wanted to do, I would’ve packed less.\n",
      "\n",
      "---\n",
      "\n",
      "I’ve grown up with abundance my entire life, so I wanted to see if I could comfortably live off of bare necessities. I found it to be a massive success.\n",
      "\n",
      "Could I have packed less? Absolutely. But this was ‘livably minimal’ — and I rarely felt that I was compromising comfort, style, or my general experience due to lack of things. Hence the massive success. **This was a setup I could sustainably live off of.**\n",
      "\n",
      "There is something fun about having funky, expressive clothing that is seldom worn. Or having the right thing for each occasion. But there is something equally enjoyable about removing the noise and clutter of unnecessary *things* in your life.\n",
      "\n",
      "Going forward, this showed me that I want to reduce the unnecessary clutter in my life — just have less stuff.\n",
      "\n",
      "Don’t get me wrong, having stuff is fun. But for the foreseeable future, I’ll be keeping things minimal.\n",
      "====================================================================================================\n",
      "\n",
      "Final philosophical prediction of your input text:\n",
      "['transcendentalism, 0.44', 'absurdism, 0.38', 'epicureanism, 0.15', 'existentialism, 0.02', 'stoicism, 0.01', 'buddhism, 0.00', 'rationalism, 0.00']\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_file = 'data/inference/rtwbackpack.txt'\n",
    "with open(text_file, 'r', encoding='utf-8') as file:\n",
    "    inference_text = file.read()\n",
    "\n",
    "\n",
    "inference_df = loader.embed_texts(inference_text, chunk_size=2000, chunk_overlap=50, print_info=False)\n",
    "philosophize_this(inference_df, model, device, label_to_category_mapping, print_info=True)\n",
    "print(f\"{'='*100}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "philosophy_oracle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
