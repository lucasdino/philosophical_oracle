{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Our Classifier for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import loader\n",
    "import models\n",
    "import utility\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_category_mapping, category_label_mapping = loader.load_labeling_mappings()\n",
    "filename_label_mapping = filename_category_mapping\n",
    "for key, value in filename_label_mapping.items():\n",
    "    filename_label_mapping[key] = int(category_label_mapping[value])\n",
    "\n",
    "label_to_category_mapping = {v: k for k, v in category_label_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model_weights\\classifier_29k_11242024_010615.pth\n"
     ]
    }
   ],
   "source": [
    "filename = 'classifier_29k_11242024_010615.pth'\n",
    "model = models.Classifier.load_model(filename).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def philosophize_this(inference_df, model, device, label_to_category_mapping, print_info=False):\n",
    "\n",
    "    def logits_to_prediction(logits, label_to_category_mapping):\n",
    "        sorted_indices = torch.argsort(logits, descending=True)    \n",
    "        predictions = [(label_to_category_mapping[str(idx.item())], logits[idx].item()) for idx in sorted_indices]\n",
    "        return predictions\n",
    "\n",
    "    full_logits = torch.zeros((len(inference_df), len(label_to_category_mapping)))\n",
    "    for i, row in inference_df.iterrows():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(row['embedding'].to(device)).cpu()\n",
    "            logits = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        full_logits[i] = logits\n",
    "        predictions = logits_to_prediction(logits, label_to_category_mapping)\n",
    "        \n",
    "        if print_info:\n",
    "            print(f\"{[f'{c}, {v:.2f}' for c, v in predictions]}\")\n",
    "            print(row['chunk_text'])\n",
    "            print(f\"{'='*100}\\n\")\n",
    "\n",
    "    final_prediction = logits_to_prediction(full_logits.mean(dim=0), label_to_category_mapping)\n",
    "    print(f\"Final philosophical prediction of your input text:\\n{[f'{c}, {v:.2f}' for c, v in final_prediction]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 2000\n",
      "['absurdism, 0.81', 'transcendentalism, 0.11', 'stoicism, 0.03', 'rationalism, 0.02', 'buddhism, 0.01', 'existentialism, 0.01', 'epicureanism, 0.01']\n",
      "This is all I took with me:\n",
      "\n",
      "4 shorts, 2 pants, 2 swimsuits\n",
      "7 shirts\n",
      "1 jacket\n",
      "6 pairs of underwear\n",
      "7 pairs of socks\n",
      "One backpack towel, hat, pair of sunglasses\n",
      "3 pairs of shoes\n",
      "Water bottle, day pack, dry sack, water bladder\n",
      "Passport, headphones, laptop, chargers, sketchbook (yellow bag), belt (blue bag)\n",
      "Couple fun things for Croatia music festival in blue bag\n",
      "Lost these Chacos when flying *(RIP)*, picked up a new shirt and shorts for Croatia at a thrift store in Barcelona, bought a pair of sunglasses from a friend’s store in Greece, and got a shirt in Byron Bay at the end of the trip. Otherwise it stayed all the same.\n",
      "If it wasn’t for Croatia and some of the long hikes I wanted to do, I would’ve packed less.\n",
      "\n",
      "---\n",
      "\n",
      "I’ve grown up with abundance my entire life, so I wanted to see if I could comfortably live off of bare necessities. I found it to be a massive success.\n",
      "\n",
      "Could I have packed less? Absolutely. But this was ‘livably minimal’ — and I rarely felt that I was compromising comfort, style, or my general experience due to lack of things. Hence the massive success. **This was a setup I could sustainably live off of.**\n",
      "\n",
      "There is something fun about having funky, expressive clothing that is seldom worn. Or having the right thing for each occasion. But there is something equally enjoyable about removing the noise and clutter of unnecessary *things* in your life.\n",
      "\n",
      "Going forward, this showed me that I want to reduce the unnecessary clutter in my life — just have less stuff.\n",
      "\n",
      "Don’t get me wrong, having stuff is fun. But for the foreseeable future, I’ll be keeping things minimal.\n",
      "====================================================================================================\n",
      "\n",
      "Final philosophical prediction of your input text:\n",
      "['absurdism, 0.81', 'transcendentalism, 0.11', 'stoicism, 0.03', 'rationalism, 0.02', 'buddhism, 0.01', 'existentialism, 0.01', 'epicureanism, 0.01']\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_file = 'data/inference/rtwbackpack.txt'\n",
    "with open(text_file, 'r', encoding='utf-8') as file:\n",
    "    inference_text = file.read()\n",
    "\n",
    "# inference_df = loader.embed_texts(inference_text, chunk_size=2000, chunk_overlap=50, print_info=False)\n",
    "# philosophize_this(inference_df, model, device, label_to_category_mapping, print_info=True)\n",
    "\n",
    "chunking_sizes = [2000]\n",
    "# chunking_sizes = [100, 200, 500, 1000]\n",
    "\n",
    "for chunk_size in chunking_sizes:\n",
    "    inference_df = loader.embed_texts(inference_text, chunk_size=chunk_size, chunk_overlap=50, print_info=False)\n",
    "    print(f\"Chunk size: {chunk_size}\")\n",
    "    philosophize_this(inference_df, model, device, label_to_category_mapping, print_info=True)\n",
    "    print(f\"{'='*100}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "philosophy_oracle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
